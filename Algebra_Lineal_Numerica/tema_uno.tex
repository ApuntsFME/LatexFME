\chapter{Errores}

\section{Errores numéricos}
	
\begin{example}\label{ej:1_errores}
    Consideramos el sistema de ecuaciones:
    \[
        \begin{pmatrix}
            1 & 2\\
            0,4999 & 1,001\\
        \end{pmatrix}
        \begin{pmatrix}
        x\\
        y\\
        \end{pmatrix} =
        \begin{pmatrix}
        3\\
        1,5\\
        \end{pmatrix}.
    \]
    Con solución $x = 1, y = 1$. Si cambiamos $0.499 \rightarrow 0.500$ para obtener el sistema
    \[
        \begin{pmatrix}
            1 & 2\\
            0,5000 & 1,001\\
        \end{pmatrix}
        \begin{pmatrix}
        x\\
        y\\
        \end{pmatrix} =
        \begin{pmatrix}
        3\\
        1,5\\
        \end{pmatrix}.
    \]
    Las solución ahora es $x = 3, y = 0$. Un cambio de $10^-3$ en uno de los parámetros ha llevado a cabo un cambio del orden del $10^1$ en la solución; este tipo de error recibe el nombre de error en los datos.
\end{example}

\begin{example}\label{ej:2_errores}
    Consideramos el calculo de las raíces del polinomio $x^2 - 18x +1$ usando aritmética de $4$ cifras decimales en el calculo de $\sqrt{\phantom{x}}$:
    \[
            x = \dfrac{18 \pm \sqrt{18^2 - 4 \cdot 1 \cdot 1}}{2 \cdot 1} = 9 \pm \sqrt{80} \nonumber.
    \]
    Si tenemos como valor para $\sqrt{80} \simeq 8.9442$, obtenemos las soluciones:
    \begin{gather*}
            x_{+}=9+8.9442=17.9442 \\
            x_{-}=9-8.9442=0.0558
    \end{gather*}.
    Hemos obtenido la primera solución con $6$ dígitos significativos, pero la segunda solo con $3$; esto es una perdida importante de información debida al error cometido en la operación $\sqrt{\phantom{x}}$ al truncar a $4$ dígitos.
\end{example}

\begin{example}\label{ej:3_errores}
    Consideramos el calculo de $I_n \coloneqq \int_{0}^{1}x^ne^{x-1}dx$, para $n = 0,1,2,...$.
    Algoritmo: obtenemos una recurrencia a partir de la formula de integración por partes
    \[
            I_n = \int_{0}^{1}x^ne^{x-1}dx = [x^ne^{x-1}]_{0}^{1} - \int_{0}^{1}nx^{x-1}e^{x-1}dx = 1 - nI_{n-1} \nonumber
    \]
    con el valor inicial $I_0 = \int_{0}^{1}x^0e^{x-1}dx = [e^{x-1}]_{0}^{1} = 1 - \frac{1}{e} \simeq 0.63212055$.
    
    Así calculamos $I_0 = 0.63212055, I_1 = 0.367899, ..., I_9 = -0.068480$; lo cual es un resultado absurdo ya que $x^ne^{x-1}$ es no negativo para $x \in [0,1]$, y por lo tanto su integral en ese intervalo también. Aunque la recurrencia sea cierta matemáticamente, el algoritmo empleado es malo numéricamente (error en el algoritmo).
\end{example}

\begin{example}\label{ej:4_errores}
    Consideramos el cálculo de la derivada de una función $f$ em el punto $x_0$
    \[
            f'\lp x_0\rp  = \limvar{h}{0} \dfrac{f\lp x_0 + h\rp  - f\lp x_0\rp }{h} \nonumber
    \]
    Sin embargo, un límite es imposible de calcular numéricamente, y por lo tanto estamos obligados a tomar una aproximación:
    \[
            f'\lp x_0\rp  \simeq \dfrac{f\lp x_0 + h\rp  - f\lp x_0\rp }{h} = \frac{\Delta f\lp x_0\rp }{h} \nonumber
    \]
    Para $h$ suficientemente pequeña, la teoría sobre límites nos dice que a medida que $h$ se acerca a $0$, el valor de la aproximación se acercará cada vez más al valor real. Desafortunadamente la teoría vuelve a fallar en la practica: si calculamos el cociente para $\frac{\Delta f\lp x_0\rp }{h}$ para $h$ sucesivamente pequeñas, el valor que obtenemos mejorará al principio pero a partir de un momento volverá a empeorar. 
\end{example}
    
\begin{defi}[Errores]\index{Error!de redondeo}\index{Error!truncado}
    Existen diferentes clases de errores según su procedencia:
    \begin{enumerate}
            \item Error de \emph{redondeo}: $\varepsilon_R$
            \begin{itemize}
                    \item Errores en los datos \ref{ej:1_errores}
                    \item Error en las operaciones \ref{ej:2_errores}
            \end{itemize}
            \item Error de \emph{truncado}: $\varepsilon_T$
            \begin{itemize}
                    \item Error en el algoritmo en el paso de contínua a aproximación discreta/numérica, \ref{ej:3_errores} y \ref{ej:4_errores}
            \end{itemize}
    \end{enumerate}
\end{defi}

\begin{defi}[Error!total]
    Definimos el error total $\varepsilon \coloneqq$ $\varepsilon_R + \varepsilon_T$.
\end{defi}

\begin{obs}
    En general, el error de tipo redondeo será alto con un número de pasos pequeño y bajo con un número de pasos más grande, mientras que el error de tipo truncado sera bajo con un número de pasos pequeño y grande con un número de pasos más grande. Debido a que el error total es la suma de los dos, no existe una solución trivial para el número de pasos óptimo $h^*$ que deberíamos emplear en los cálculos; el minimizador del error total que la teoría asegura que existe.
\end{obs}

\section{Representación digital de un número real}
    
\begin{prop}
    Todo $x \in \R$ admite una representación única en base $b \in \n$, $b>1$ con dígitos $\lp  a_j\rp  _{j \in \z}$ tal que 
    \[
            x = \cdots + a_kb^k + \cdots a_0b^0 + a_{-1}b^{-1} + \cdots \nonumber.
    \]
\end{prop}

\begin{obs}
    En general la secuencia de dígitos $a_j$ tendrá un numero infinito de valores distintos de cero. Al ser la capacidad de los ordenadores finita, es imposible representar todos los reales con precisión exacta, lo cual generará errores.
\end{obs}

\begin{defi}[Representación!normalizada]\index{Representación!única}
    La representación normalizada o única de $x \in \real$ en base $b$ con dígitos $\lp  d_n\rp $, $n \in \z^{+}$ y exponente $e \in \z$ es
    \[
      x = \pm 0,d_1d_2... \cdot b^e \nonumber,
    \]
    con la restricción de que $d_1 \neq 0$.
\end{defi}

\begin{example}
  Representación normalizada de $172.456$ en base $10$ es $\SI{0.172456e3}{}$.
\end{example}

\begin{defi}[Representación!en coma flotante]
	La representación en coma flotante con $n$ dígitos de un numero real $x$ con representación normalizada $x = \pm 0d_1..d_n... \cdot b^e$ es:
	\begin{itemize}
		\item  Por truncado:
		\[
			\fl_T\lp x\rp  = \pm 0.d_1..d_n \cdot b^e \nonumber
		\]
		\item Por redondeo:
			\[
			\fl\lp x\rp  = \left.
			\begin{cases}
			\pm 0.d_1..d_n \cdot b^e & 0 \leq d_n < \frac{b}{2} \\
			\pm \lp 0.d_1..d_n + b^{-n}\rp  \cdot b^e & \frac{b}{2} \leq x < b
			\end{cases}
			\right.
			\]
	\end{itemize}
\end{defi}

\begin{obs}
  Supongamos que queremos almacenar $\fl\lp x\rp $ con la siguiente estructura en base $b = 2$:
  \[
	  \underbrace{[*]}_{s_1}
	  \hspace{0.1cm}
	  \underbrace{[{}*|*|*|*|*|*|*{}]}_{d_1,...,d_n}
	  \hspace{0.1cm}
	  \underbrace{[*]}_{s_2}
	  \hspace{0.1cm}
	  \underbrace{[{}*|*|*|*|*{}]}_{e_1,..,e_k}
	  \nonumber
  \]

  Donde $s_1$ es el signo de $x$, $d_1,...,d_n$ son los dígitos de $\fl\lp x\rp $, $s_2$ es el signo del exponente y $e_1,...,e_k$ los dígitos del exponente de $\fl\lp x\rp $. Sin embargo, este mecanismo de representación no es del todo eficiente.

  Una primera mejora se basaría en darnos cuenta de que, al ser la base $2$ y $d_1 \neq 0$, necesariamente $d_1 = 1$. Así pues, no hay razón para guardarlo y malgastar memoria. Esto nos da un dígito más de precisióncon el mismo espacio.

  Una segunda mejora sería cambiar la manera en el que guardamos el exponente $e$. Definimos $e' = e + n^{\underline{\text{o}}}$, con un $n^{\underline{\text{o}}}$ fijo, definido como el mínimo natural tal que $e' > 0$ para todos los $e$ posibles en la representación original. Observamos que la definición de $e'$ establece una biyección, así que guardando $e'$ obtenemos unívocamente $e$. Al ser $e' > 0$ siempre, al guardarlo podemos prescindir del bit asignado al signo de $e$ i utilizarlo para guardar un dígito más de $e'/e$. El valor de $n^{\underline{\text{o}}}$ para el tipo float ($32$ bits, $23$ para la mantisa, $8$ para el exponente) es $127$, y $1023$ para el tipo double ($64$ bits, $52$ para la mantisa, $11$ para el exponente).
\end{obs}

\begin{defi}[Épsilon!máquina]
	El $\varepsilon$-máquina de un ordenador es el mínimo $\varepsilon > 0$ tal que $\fl\lp 1 + \varepsilon\rp  > 1$.
\end{defi}
        
\section{Errores en la representación de los números}

\begin{defi}[Error!absoluto]\index{Error!relativo}
    Sea $x \in \R$ un número real y $\overline{x}$ una aproximación de $x$
    \begin{itemize}
	\item Definimos el error absoluto cometido en aproximar $x$ con $\overline{x}$ como $e_a\lp \overline{x}\rp  = \overline{x} - x$.
	\item Definimos el error relativo cometido en aproximar $x$ con $\overline{x}$ como
	$e_r\lp \overline{x}\rp  = \frac{e_a\lp \overline{x}\rp }{x} \simeq \frac{e_a\lp \overline{x}\rp }{\overline{x}}$.
    \end{itemize}
\end{defi}

\begin{obs}
    Como los errores no suelen ser conocidos, manejaremos cotas superiores de ellos:    \begin{align*}
	\overline{x} &= x + e_a\lp \overline{x}\rp  & \abs{e_a\lp \overline{x}\rp } \leq \varepsilon_a\lp \overline{x}\rp  \text{ cota del error absoluto} \\
	\overline{x} &= x\lp 1 + \delta\rp  & \abs{e_r\lp \overline{x}\rp }=\abs{\delta} \leq \varepsilon_r\lp \overline{x}\rp  \text{ cota del error relativo}.
    \end{align*}
\end{obs}

\begin{obs}
    $\overline{x} = x\lp 1 + \delta\rp  = x + \underbrace{\delta x}_{e_a\lp \overline{x}\rp }$
\end{obs}
  
\begin{prop} 
  Sea $x = \pm 0.d_1..d_nd_{n+1}... \cdot b^e$. Entonces, las cotas para el error absoluto y relativo al hacer punto flotante por redondeo son
  \begin{align*}
    e_a\lp \fl_R\lp x\rp \rp  &=  \frac{1}{2}b^{-n+e} \\
    e_r\lp \fl_R\lp x\rp \rp  &=  \frac{1}{2}b^{-n+1}.
  \end{align*}
\end{prop}
\begin{proof}
   La representación de $x$ en coma flotante con $n$ dígitos por truncado es $\fl\lp x\rp  = \pm 0.d_1..d_n$. Entonces por definición tenemos que
    \begin{align*}
            |e_a\lp \fl_T\lp x\rp \rp | &= |\fl\lp x\rp  - x| = d_{n+1}... \cdot b^{-\lp n+1\rp } \cdot b^e \leq b \cdot b^{-\lp n+1\rp } \cdot b^e &= b^{-n+e} = \epsilon_a\lp \fl_T\lp x\rp \rp  \\
            |e_r\lp \fl_T\lp x\rp \rp | &= |\frac{e_a\lp \overline{x}\rp }{x}| \leq \frac{b^{-n+e}}{b^{-1+e}} &= b^{-n+1} = \epsilon_r\lp \fl_T\lp x\rp \rp .
    \end{align*}
    Con un argumento similar al anterior vemos que las cotas para el error absoluto y relativo al hacer punto flotante por redondeo son
    \begin{align*}
            \abs{e_a\lp \fl_R\lp x\rp \rp } &=  \frac{1}{2}b^{-n+e} = \varepsilon_a\lp \fl_R\lp x\rp \rp  \\
            \abs{e_r\lp \fl_R\lp x\rp \rp } &=  \frac{1}{2}b^{-n+1} = \varepsilon_r\lp \fl_R\lp x\rp \rp 
    \end{align*}
\end{proof}
 
\begin{defi}[cifras!decimales correctas]
    Considerando $b = 10$ definimos:
    \begin{itemize}
	\item Si $e_a\lp \overline{x}\rp  \leq \frac{1}{2}10^{-k}$, diremos que $\overline{x}$ tiene $k$ cifras decimales correctas.
	\item Si $e_r\lp \overline{x}\rp  \leq \frac{1}{2}10^{-k+1}$, diremos que $\overline{x}$ tiene $k$ dígitos significativos corectos.
    \end{itemize}
\end{defi}

\begin{obs}
  En esta definición aparecen los $\frac{1}{2}$ ya que por defecto consideramos coma flotante por redondeo al ser el error menor. En caso de utilizar coma flotante por truncado, las definiciones serían iguales sin el $\frac{1}{2}$.
\end{obs}

\section{Propagación de errores en los datos}

\begin{prop}
  Como propiedades fundamentales de la propagación de errores tenemos
  \begin{enumerate}[i)]
    \item $\varepsilon_a\lp \overline{x} \pm \overline{y}\rp  = \varepsilon_a\lp \overline{x}\rp  + \varepsilon_a\lp \overline{y}\rp$.
    \item $\varepsilon_r\lp \overline{x}\cdot\overline{y}\rp  \lessapprox \varepsilon_r\lp \overline{x}\rp  + \varepsilon_r\lp \overline{y}\rp$.
  \end{enumerate}
\end{prop}
\begin{proof}
  \begin{enumerate}[i)]
   \item []
   \item Directo de la definición.
   \item Tenemos que $e_r\lp \overline{x}\cdot\overline{y}\rp  = xy\lp 1+\delta_x\rp \lp 1+\delta_y\rp  = xy\lp 1 + \delta_x + \delta_y + \delta_x\delta_y\rp  \simeq xy\lp 1 + \delta_x + \delta_y\rp$ y si $\delta_x,\delta_y << 1$, entonces $\delta_x\delta_y << \delta_x + \delta_y$ y tenemos que $\varepsilon_r\lp \overline{x}\cdot\overline{y}\rp  \lessapprox \varepsilon_r\lp \overline{x}\rp  + \varepsilon_r\lp \overline{y}\rp$.
  \end{enumerate}
\end{proof}

\begin{obs}
    Ojo con restar variables \emph{cercanas}, se generaran errores relativos grandes.
    \[
	e_r\lp \ap{x}-\ap{y}\rp  = \dfrac{e_a\lp \ap{x} - \ap{y}\rp }{\ap{x}-\ap{y}} = \dfrac{\ap{x}}{\ap{x}-\ap{y}}e_r\lp \ap{x}\rp  - \dfrac{\ap{y}}{\ap{x}-\ap{y}}e_r\lp \ap{y}\rp.
    \]
    Si ahora tomamos el valor absoluto,
    \[
	\abs{e_r\lp \ap{x}-\ap{y}\rp} \leq \abs{\dfrac{\ap{x}}{\ap{x}-\ap{y}}}\cdot\abs{e_r\lp \ap{x}\rp} + \abs{\dfrac{\ap{y}}{\ap{x}-\ap{y}}}\cdot\abs{e_r\lp \ap{y}\rp}.
    \]
    Vemos que si $\ap{x}$, $\ap{y}$ son cercanos, $\ap{x}-\ap{y}$ será muy pequeño y al dividir la cota del error incrementa mucho.
\end{obs}

\begin{example}
	Calcular las raízes del polinomio  $x^2 - 18x + 1$, con calculo de $\sqrt{\phantom{x}}$ de 4 cifras decimales por truncado del ejemplo \ref{ej:2_errores}.
	\begin{align*}
		x_+ &= 9 + \sqrt{80} = 17.94442 &&\text{ 6 dígitos significativos} \\
		x_- &= 9 - \sqrt{80} = \enspace 0.0558 &&\text{ 3 dígitos significativos}.
	\end{align*}
	Analizamos este resultado en terminos de la cota del error relativo en el calculo de $x_-$: Sea $x = 9$, $y = \sqrt{80}$, $\ap{x}=9$, $\ap{y}=8.4492$. Entonces tenemos que:
	\[
		e_r\lp \ap{x}-\ap{y}\rp  \simeq \abs{\frac{\ap{y}}{\ap{x}-\ap{y}}}\cdot\abs{e_r\lp \ap{y}\rp} = \dfrac{8.9442}{0.0588}\cdot \underbrace{10^{-5+1}}_{\text{5 díg sig}} \simeq \scin{0.16e-1} < 1^{-2+1}
	\]
	Es decir, me puedo creer solo $\approx 2$ dígitos significativos.
\end{example}

\begin{example}
	Consideramos el calculo de $I_n = \int_{0}^{1}x^ne^{x-1}dx$,$n = 0,1,2,...$ del ejemplo \ref{ej:3_errores}, y para ese fín el uso de la recurrencia $I_n = 1 - nI_{n-1}$, $I_0 = 1 - \frac{1}{e} \approx 0,63212055$. Para $I_9$ obteníamos un valor negativo (cuando la función era positiva en el intervalo de integración) . Si analizamos este problema en términos del error absoluto: $e_a\lp I_n\rp  \lessapprox e_a\lp 1\rp  + ne_a\lp I_{n-1}\rp $, es decir $e_a\lp I_n\rp  \simeq n!\,e_a\lp I_0\rp $. El error absoluto aumenta factorialmente, por lo tanto, este procedimiento no es bueno.
\end{example}

\begin{prop}\label{prop:1_error_funcion}
  Sea $x \in \real$, $\overline{x}$ una aproximación de $x$ y $f:\: \real \to \real$, $f\in \C^1$, una función. Entonces,
  \begin{enumerate}[i)]
   \item $\abs{e_a\lp f\lp\overline{x}\rp\rp} \simeq \abs{f'\lp \overline{x}\rp}\abs{e_a\lp x\rp}$
   \item $\abs{e_r\lp f\lp\overline{x}\rp\rp} \simeq \abs{\frac{\overline{x}f'\lp\overline{x}\rp}{f\lp\overline{x}\rp}}\abs{e_r\lp x\rp}$.
  \end{enumerate}
\end{prop}
\begin{proof}
  \begin{enumerate}
   \item []
   \item 
      $e_a\lp f\lp \overline{x}\rp\rp  = e_a\lp f\lp x + e_a\lp x\rp \rp\rp \simeq e_a\lp f\lp x\rp  + f'\lp x\rp e_a\lp x\rp\rp = \abs{f'\lp \overline{x}\rp}\abs{e_a\lp x\rp}$.
   \item 
      $e_r\lp f\lp \overline{x}\rp \rp  = \frac{e_a\lp f\lp \overline{x}\rp \rp }{f\lp \overline{x}\rp } \simeq \frac{f'\lp \overline{x}\rp e_a\lp x\rp }{f\lp \overline{x}\rp } = \frac{\overline{x}f'\lp \overline{x}\rp }{f\lp \overline{x}\rp }e_r\lp x\rp$.
  \end{enumerate}
\end{proof}

\begin{defi}[factor!de propagación]
   Sea $x \in \real$, $\overline{x}$ una aproximación de $x$ y $f:\: \real \to \real$, $f\in \C^1$, una función. Entonces definimos el factor de propagación del error (f.d.p) por $f$ como $\alpha = \abs{\frac{\overline{x}f'\lp\overline{x}\rp}{f\lp\overline{x}\rp}}$.
\end{defi}

\begin{example}[Seguridad de la operación raíz cuadrada]
      Consideramos el cálculo de $f\lp x\rp  = \sqrt{x}$. Tenemos que en ese caso f.d.p = $\frac{x\frac{1}{2\sqrt{x}}}{\sqrt{x}} = \frac{1}{2}$. Por lo tanto,  $\abs{e_r\lp \sqrt{\overline{x}}\rp} \simeq \frac{1}{2}\abs{e_r\lp \overline{x}\rp}$. Lo que nos dice que calcular la raíz cuadrada es una operación que no aumenta el error.
\end{example}

\begin{example}[Seguridad de la operación $\frac{1}{1-x^2}$]
      Consideramos el cálculo de $f\lp x\rp  = \frac{1}{1-x^2}$. Entonces tenemos que f.d.p = $\frac{x\frac{4x}{\lp 1-x^2\rp ^2}}{\frac{1}{1-x^2}} = \frac{4x^2}{1-x^2} = 4\lp 1 - \frac{1}{1-x^2}\rp $. Vemos que el f.d.p será muy grande cuando $x \simeq 1$, y en ese caso, calcular $f$ de esta manera sería una operación insegura. En vez de calcularlo dividiendo, podemos expandir en Taylor: $f\lp x\rp  \equiv 1 + x^2 + x^4 + x^6 + ...$ y entonces hacer el cálculo.
\end{example}

\begin{example}[Efecto en el cambio de los coeficientes de un sistema lineal en la solución]
      
    Consideramos el sistema de ecuaciones:
    \[
	\begin{pmatrix}
	    a & b\\
	    c & d\\
	\end{pmatrix}
	\begin{pmatrix}
	x\\
	y\\
	\end{pmatrix} =
	\begin{pmatrix}
	3\\
	1,5\\
	\end{pmatrix}.
    \]
    Con soluciones $x = \frac{3d-1.5b}{ad-bc}$,$y = \frac{1.5a-3c}{ad-bc}$. Supongamos que sabemos $a = 1,b = 2,d = 1.001$ de manera exacta, pero solo tenemos una aproximación de $c$ dada por $\overline{c} = 0.499$, con error absoluto $e_r\lp c\rp  \simeq \scin{1e-3}$. Al resolver este sistema, nosotros calculamos $\overline{x} = \frac{3d-1.5b}{ad-b\overline{c}} \cong f\lp \overline{c}\rp $. El error absoluto cometido en el cálculo de $\overline{x}$ está dado por
    \[
	    \abs{e_a\lp \overline{x}\rp} = \abs{e_a\lp f\lp \overline{c}\rp \rp} \simeq \abs{f'\lp \overline{c}\rp}\abs{e_a\lp \overline{c}\rp}.
    \]
    Donde
    \[
	    \abs{f'\lp \overline{c}\rp}= \abs{\frac{\lp 3\cdot 1.001-1.5\cdot 2\rp \cdot 2}{\lp 1\cdot 1.001 -2\cdot\overline{c}\rp ^2}}_{\overline{c}=0.499} \simeq \scin{6e3}.
    \]
    Y por lo tanto, $\abs{e_a\lp x\rp}\simeq 6$, lo que nos dice que para estos valores tenemos una operación insegura.
\end{example}

\begin{obs}
      Supongamos que queremos calcular $f\lp x\rp  \equiv f\lp x_1,...,x_n\rp $, pero solo tenemos $\overline{x} = \lp \overline{x_1},...,\overline{x_n}\rp $. Entonces el error cometido, suponiendo que el cálculo de $f$ es perfecto, viene dado por
      \[
	      f\lp \overline{x}\rp  = f\lp x + e_a\lp x\rp \rp  = f\lp x\rp  + \nabla f\lp x\rp^Te_a\lp x\rp  + \theta_2\lp e_a\lp x\rp \rp.
      \]
      Entonces,
      \[
	      e_a\lp f\lp \overline{x}\rp \rp  \simeq \nabla f\lp x\rp ^Te_a\lp x\rp  = \sum_{k=1}^{n}\pdv{f}{x_k} \lp \overline{x}\rp e_a\lp \overline{x}_k\rp.
      \]
\end{obs}

\begin{prop}
  Sea $x \in \real$, $\overline{x}$ una aproximación de $x$ y $f:\: \real \to \real$, $f\in \C^1$, una función. Sea $\delta_f$ una cota del error de la función tal que $\abs{f(x)-\overline{f}(x)}<\delta_f$, $\forall x \in \real$. Entonces tenemos la siguiente expresión para el error cometido
  \[
      e_a\lp \overline{f}\lp \overline{x}\rp \rp  \simeq \abs{\overline{f}'\lp \overline{x}\rp }\abs{e_a\lp x\rp} + \delta_f\abs{\overline{f}\lp \overline{x}\rp}.
  \]
\end{prop}
\begin{proof}
  Igual que la demostración de la proposición \ref{prop:1_error_funcion}.
\end{proof}

\section{Cálculo de raízes de un polinomio}

\begin{comment} 

    
    
    Consideramos las raíces de un polinomio como una función implícita de sus coeficientes. Veremos que es una relación muy sensible y mal condicionada. 
    
    \subsection{Análisis de un polinomio general}
    
    Consideramos el polinomio general:
    \[
            p\lp x\rp  = a_nx^n + \cdots a_kx^k + \cdots a_1x + a_0
    \]
    
    \begin{itemize}[label=\lp $\bullet$\rp ]
            \item Sabemos que las raíces de $p\lp x\rp $ son $\alpha_1,...,\alpha_n$
            \item También sabemos que cada raíz $\alpha_l$ es función de los coeficientes: $\alpha_l = \alpha_l\lp \ap{a}_n,...,\ap{a}_0\rp $
            \item Queremos estudiar $e_a\lp \alpha_l\lp \ap{a}_n,...,\ap{a}_0\rp \rp $, $e_r\lp \lp \alpha_l\lp \ap{a}_n,...,\ap{a}_0\rp \rp $ $l = 1\divisionsymbol n$
    \end{itemize}

    En general sabemos, de la fórmula del factor de propagación del error:
    \[
    e_a\lp \alpha_l\lp \ap{a}_n,...,\ap{a}_0\rp \rp  \simeq \sum_{j=1}^{n}\dv{\alpha_l}{a_j}e_a\lp \ap{a}_j\rp 
    \]
    
    En nuestro ejemplo supondrems que solo hay error en el coeficiente $a_k$. Por lo tanto:
    \[
            e_a\lp \alpha_l\lp \ap{a}_k\rp \rp  \simeq \pdv{\alpha_l}{a_k}\lp a_k\rp e_a\lp \ap{a}_k\rp 
    \]
    
    Necesitamos calcular $\pdv{\alpha_l}{a_k}\lp a_k\rp $. Utilizaremos derivación implícita respecto $a_k$ sobre la siguiente igualdad:
    \[
            a_n\alpha_l^n + \cdots a_k\alpha_l^k + \cdots a_0 = p\lp \alpha_l\rp \rp  = p\lp \alpha_l\lp a_k\rp \rp 
    \]
    
    Para obtener:
    \[
            \alpha_l^k = p'\lp \alpha_l\lp a_k\rp \rp \pdv{\alpha_l}{a_k}\lp a_k\rp  \implies \pdv{\alpha_l}{a_k}\lp a_k\rp  = \dfrac{\alpha_l^k}{p'\lp \alpha_l\lp a_k\rp \rp }
    \]
    
    El numerador de la expresión obtenida es fácil de calcular; para calcular el denominador haremos uso de la representación de un polinomio como producto de sus factores simples:
    \begin{align*}
            p\lp x\rp  & = a_n\lp x-\alpha_l\rp \prod_{j=1,j\neq l}^{n}\lp x-\alpha_j\rp  \\
            \implies p'\lp x\rp  & = a_n\prod_{j=1,j\neq l}^{n}\lp x-\alpha_j\rp  + a_n\lp x-\alpha_l\rp \dv{x}\left\lp \prod_{j=1,j\neq l}^{n}\lp x-\alpha_j\rp \right\rp  \\
            \therefore p'\lp \alpha_l\rp  & = a_n\prod_{j=1,j\neq l}^{n}\lp \alpha_l-\alpha_j\rp 
    \end{align*}
    
    De manera que obtenemos la expresión deseada para le error absoluto del calculo de $\alpha_l$ debido al error en $\ap{a}_k$:
    \[
            \therefore |e_a\lp \alpha_l\lp \ap{a}_k\rp \rp | \simeq \left|\frac{\alpha_l^k}{ a_n\prod_{j=1,j\neq l}^{n}\lp \alpha_l-\alpha_j\rp }\right||e_a\lp \ap{a}_k\rp |
    \]
    
    Con respecto al error relativo:
    \begin{align*}
            e_r\lp \alpha_l\lp \ap{a}_k\rp \rp  & = \left|\frac{e_a\lp \alpha_l\lp \ap{a}_k\rp \rp }{\alpha_l\lp \ap{a}_k\rp }\right| \simeq \left|\frac{\alpha_l^{k-1}}{ a_n\prod_{j=1,j\neq l}^{n}\lp \alpha_l-\alpha_j\rp }\right|\left|\frac{e_a\lp \ap{a}_k\rp }{a_k}\right||a_k| \\
            & = \left|\frac{a_k\alpha_l^{k-1}}{a_n\prod_{j=1,j\neq l}^{n}\lp \alpha_l-\alpha_j\rp }\right| |e_r\lp \ap{a}_k\rp |
    \end{align*}
    
    Hasta aquí en general. En nuestro caso particular de Wilkinson, tenemos $n=20$,$\alpha_1 = 1$,...,$\alpha_{20} = 20$,$k = 19 \leftrightarrow a_{19} = -210$,$l = 16 \leftrightarrow \alpha_l = 16$. Calculamos el error relativo:
    
    \begin{align*}
                    e_r\lp \alpha_l\lp \ap{a}_k\rp \rp  = \left|\frac{\lp -210\rp \cdot\lp 16^{19-1}\rp }{1\cdot\lp 15!4!\lp -1\rp ^4\rp }\right| |e_r\lp \ap{a}_{19}\rp | \approx \scin{0.31e11}e_r\lp \ap{a}_{19}\rp 
    \end{align*}
    
    Consequencia: Suponemos que queremos calcular el nuevo $\alpha_l\lp \ap{a}_k\rp $ \lp ex. $\alpha_{16}\lp \ap{a}_{19}\rp $\rp  con $t$ dígitos significativos. ¿Cuantos dig. sig. correctos debería tener, aproximadamente, $a_{19}$?
    
    \begin{align*}
            & \frac{1}{2}\cdot10^{1-t} \sim \scin{0.31e11}\frac{1}{2}\cdot10^{1-s} \\
            \therefore & 1 - t = 12 - s \iff \\
            & s \sim 11 + t
    \end{align*}
    
    Es decir que si queremos un valor para la raíz que nos podamos creer $t$ dígitos, entonces necesitamos saber $11 + t$ dígitos del coeficiente $a_{19}$ \lp suponiendo que conocemos los otros de manera exacta\rp .
    
    \section{Problema: ''No asociatividad de la suma en numérica"}
    
    Suponemos $\delta$ cota del error relativo en el almacenamiento \lp $\delta \sim \frac{1}{2}10^{1-t} \equiv t$ cifras significativas\rp . Queremos analizar las diferencias entre: $\sum_{n=1}^{N}\frac{1}{n^2}$ vs $\sum_{n=N}^{1}\frac{1}{n^2}$. Calculamos primero el error cometido al sumar una secuencia general de $n$ términos:
    
    \begin{table}[H]
            \centering
            \label{no-asociativa}
            \begin{tabular}{lll}
                    Valor real & Aproximación & Cota del error absoluto \\
                    $s_1 = x_ 1$ & $\ap{s_1} = \ap{x_1}$ & $\varepsilon\lp \ap{s_1}\rp  = \varepsilon_a\lp \ap{x_1}\rp  = \ap{x_1}\delta$ \\
                    $s_2 = s_1 + x_ 2$ & $\ap{s_2} = \ap{s_1} + \ap{x_2}$  &   $\varepsilon_a\lp \ap{s_2}\rp  \simeq \lp 2x_1 + 2x_2\rp \delta$  \\
                    $s_3 = s_2 + x_3$ & $\ap{s_3} = \ap{s_2} + \ap{x_3}$   &   $\varepsilon_a\lp \ap{s_3}\rp  \simeq \lp 3x_1 + 3x_2 + 2x_3\rp \delta$\\
                    $s_4 = s_3 + x_4$ & $\ap{s_4} = \ap{s_3} + \ap{x_4}$   &   $\varepsilon_a\lp \ap{s_4}\rp  \simeq \lp 4x_1 + 4x_2 + 3x_3 + 2x_4\rp \delta$ \\
                    $s_N = s_{N-1} + x_N$ & $\ap{s_N} = \ap{s_{N-1}} + \ap{x_N}$ & $\varepsilon_a\lp \ap{s_N}\rp  \simeq \lp Nx_1 + Nx_2 + \lp N-1\rp x_3 + \lp N-2\rp x_4 + \cdots + 2x_N\rp \delta$
            \end{tabular}
    \end{table}

    Derivación de las cotas \lp casos $s_1$,$s_2$; las demás se obtienen análogamente\rp :

    \begin{align*}
            \lp 1\rp  : & \varepsilon_a\lp \ap{s_2}\rp  = \varepsilon_a\lp \ap{s_1}\rp  + \varepsilon_a\lp x_2\rp  + \lp \ap{s_1} + \ap{x_2}\rp \delta = x_1\delta + x_2\delta + \lp \ap{x_1} + \ap{x_2}\rp \delta \simeq \lp 2x_1 + 2x_2\rp \delta \\
            \lp 2\rp  : & \varepsilon_a\lp \ap{s_3}\rp  = \varepsilon_a\lp \ap{s_2}\rp  + \varepsilon_a\lp x_3\rp  + \lp \ap{s_2} + \ap{x_3}\rp \delta \simeq \lp 2x_1 + 2x_2\rp \delta + x_3\delta + \lp x_1 + x_2 + x_3\rp \delta = \lp 3x_1 + 3x_2 + 2x_3\rp \delta \\
    \end{align*}
    
    Para minimizar el error, querríamos: $x_1 \leq x_ 2 \leq x_3 \leq \cdots \leq x_N$. En nuestro caso particular con $x_j \coloneqq \frac{1}{j^2}$, este resultado nos indica que la suma inversa \lp $\sum_{j=N}^{1}\frac{1}{j^2}$\rp  daría un resultado mejor.
    \end{comment}
    