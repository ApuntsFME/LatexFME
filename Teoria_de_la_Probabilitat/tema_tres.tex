\chapter{Variables aleatòries discretes}

\section{Definició i objectes relacionats}

\begin{defi}[variable aleatòria!discreta]
    Sigui $\lp \Omega, \Asuc, p\rp$ un espai de probabilitat i sigui $X\colon \Omega\to\real$ una variable aleatòria. Diem que $X$ és discreta si
    $\im(X)$ és numerable.
\end{defi}

\begin{obs}
    En la pràctica, $\im(X) = \lc x_1 < x_2 < \dots \rc$ és un conjunt numerable ordenat. En els casos que veurem, $\im(X)\subseteq\z$.
    Escriurem $p_i = p(X=x_i)$.
\end{obs}

\begin{obs}
    Sigui $X$ una variable aleatòria discreta i sigui $A\in\mathcal{B}$. Aleshores, 
    \[
        p_X(A) = p(x\in A) = \sum_{x_i\in A\cap\im\lp X\rp}{p_i}.
    \]
\end{obs}

\begin{obs}
    Donat $\lc x_i \rc_{i \ge 1} \subset\real$ creixent i valors $\lc p_i \rc_{i \ge 1} \subset [0,1] \tq \sum{p_i}=1$, es pot definir
    una variable discreta $X$ que pren valors a $\lc x_i \rc$ tal que $p(X=x_i) = p_i$ $\forall\,i$.
\end{obs}

\begin{obs}
    La funció de distribució, amb $|\im(X)|<+\infty$, és
    \[F_X(x) = \begin{cases}
        0 &\text{si } x<x_1\\
        p_1+\cdots+p_j &\text{si } x_j \le x < x_{j+1}\\
        1 &\text{si } x \ge x_n
    \end{cases}.\]
\end{obs}

\begin{prop}[Operador esperança] Sigui $X$ una variable aleatòria discreta. Aleshores,
    \begin{enumerate}[i)]
        \item $\esp[X] = \sum\limits_{i \ge 1}{x_i p_i}$,
        \item Si $g\colon \real\to\real$ és una funció mesurable, $\esp[g(x)] = \sum\limits_{i \ge 1}{g(x_i) p_i}$.
    \end{enumerate}
\end{prop}

\begin{proof}
    \begin{enumerate}[i)]
        \item[]
        \item Fent servir la definició d'esperança tenim que
	  \[ \esp[X] = \int_{\real}{x\dif P_X} = \sum{x_i P_X(X=x_i) + \int_{\real\setminus\cup\{x_i\}}{x\mathbb{I}(x) \dif P_X}} = \sum_{i \ge 1}{x_i p_i} + 0\]
            perquè $P_X(\real\setminus\cup\{x_i\}) = 0$.
        \item Directe a partir del cas anterior i del fet que $g$ és mesurable.
    \end{enumerate}
\end{proof}

\begin{obs}
    Sigui $X$ una variable aleatòria. Aleshores,
    \[\var[X] = \esp[X^2] - \esp[X]^2 = \sum_{i \ge 1}{x_i^2 p_i} - \left(\sum_{i \ge 1}{x_i p_i}\right)^2.\]
\end{obs}

\begin{prop}
    Siguin $X, Y$ variables aleatòries discretes, amb $\im(X) = \lc x_i \rc_{i \ge 1}$ i $\im(Y) = \lc y_i \rc_{i \ge 1}$, i sigui $g\colon \real^2\to\real$. Aleshores
    \[\esp[g(X,Y)] = \sum_{i,j \ge 1}{g(x_i,y_i) p(X=x_i, Y=y_i)}.\]
\end{prop}
\begin{proof}
    Directe a partir de la definició i del fet que $g$ és mesurable.
\end{proof}


\begin{prop}
    Siguin $X,Y$ variables aleatòries discretes. Són independents si i només si $\forall x \in\im(X)$ i $\forall y \in\im(Y)$,
    \[P(X=x, Y=y) = P(X=x)P(Y=y)\]
\end{prop}
\begin{proof}
    Exercici.
\end{proof}


\begin{defi}[vector de variables aleatòries!discret]
    Sigui $(X,Y)\colon \Omega\to\real^2$ un vector de variables aleatories. Direm que és discret si $\im((X,Y))$ és numerable.
\end{defi}

\begin{obs}
    Sigui $(X,Y)$ vector de variables aleatories. Aleshores és discret si i només si $X$ i $Y$ són discretes.
\end{obs}

\begin{defi}
    Sigui $(X,Y)$ vector de variables aleatories discret. Definim
    \begin{align*}
        P_{\lp X,Y\rp}\colon\real ^2&\to\real\\
        \lp x, y\rp &\mapsto P_{\lp X,Y\rp}\lp x, y\rp \equiv P\lp X=x\rp P\lp Y=y\rp.
    \end{align*}
    Si $X,Y$ són independents, $P_{\lp X,Y\rp}\lp x, y\rp=P(X=x,Y=y)$.
\end{defi}

\begin{lema}
    Si $X,Y$ són variables aleatòries discretes independents amb $\esp[|X|], \esp[|Y|] < +\infty$,
    \[\esp[XY] = \esp[X]\esp[Y].\]
\end{lema}

\begin{proof}
    \begin{align*}
        \esp[XY] &= \sum_{a\in\im\lp X\rp\cup\im\lp Y\rp} aP\lp XY=a\rp=\\
        &=\sum_{a\in\im\lp X\rp\cup\im\lp Y\rp} a\sum_{b\in\im\lp X\rp\setminus\lc 0\rc} P\lp X=b, Y=\frac{a}{b}\rp=\\
        &=\sum_{a\in\im\lp X\rp\cup\im\lp Y\rp} a\sum_{b\in\im\lp X\rp\setminus\lc 0\rc} P\lp X=b\rp P\lp Y=\frac{a}{b}\rp=\\
        &=\sum_{b\in\im\lp X\rp\setminus\lc 0\rc}\sum_{a\in\im\lp X\rp\cup\im\lp Y\rp} a P\lp Y=\frac{a}{b}\rp P\lp X=b\rp=\\
        &=\sum_{b\in\im\lp X\rp\setminus\lc 0\rc} bP\lp X=b\rp\sum_{a\in\im\lp X\rp\cup\im\lp Y\rp} \frac{a}{b} P\lp Y=\frac{a}{b}\rp=\\
        &=\sum_{b\in\im\lp X\rp\setminus\lc 0\rc} bP\lp X=b\rp\esp\lb Y\rb=\\
        &=\esp\lb X\rb\esp\lb Y\rb.
    \end{align*}
\end{proof}


\section{Funció generadora de probabilitat}
D'aquí en endavant, prendrem $X$ variable aleatòria discreta amb $\im(X) \subseteq \n_{\geq 0}$.
\begin{defi}[funció!generadora de probabilitat]
    Definim la funció generadora de probabilitat d'$X$ com la sèrie formal de potències
    \[G_X(z) = \sum_{n \geq 0} P(X = n) z^n.\]
    Podem pensar-la també com $\esp[z^X]$.
\end{defi}

\begin{prop}
    $G_X(z)$ satisfà les següents propietats:
    \begin{enumerate}[i)]
    \item $G_X(z)$ \'es una funció holomorfa al voltant de $z = 0$ amb radi de convergència
        major o igual a $1$.
    \item $G_X(0) = P(X = 0)$ i $G_X(1)$ = 1.
    \item $\eval{\frac{\text{d}^kG_X(z)}{\text{d}z}}_{z = 1} = 
        \esp\lb X(X-1)\dots(X-k+1)\rb = \esp \lb (X)_k\rb $.
    \end{enumerate}
\end{prop}

\begin{proof}
    \begin{enumerate}[i)]
        \item[]
        \item Si $\rho \in \cx$, $|\rho| < 1$, aleshores:
            \[
                0 \leq |G_X(\rho)| = \left|\sum_{n \geq 0} P(X = n) \rho^n \right|
                \leq \sum P(X=n) |\rho|^n
            \]
            que, quan $|\rho| \leq 1$, \'es menor o igual a
            \[
                \sum_{n \geq 0} P(X = n) = 1.
            \]
            Per tant $G_X(\rho)$ \'es analítica (es pot expressar com una sèrie de
            potències convergent) a $B_1(0)$, i per tant, com s'ha vist a variable
            complexa, $G_x(\rho)$ \'es holomorfa a $B_1(0)$ (per tant infinitament
            derivable en sentit complex).
        \item  Directe a partir de la definició.
        \item Si derivem terme a terme obtenim
            \[
                \frac{\text{d}^kG_X(z)}{\text{d}z} = \frac{\text{d}^k}{\text{d}z} \lp \sum_{n \geq 0} P(X = n) z^n\rp=
                \sum_{n \geq 0} n(n-1)\dots(n-k+1)P(X = n) z^{n-k},
            \]
            que avaluat en $z = 1$ \'es
            \[ 
                \sum_{n \geq 0} n(n-1)\dots(n-k+1)P(X=n) = \esp\lb(X)_n\rb.
            \]
    \end{enumerate}
\end{proof}

\begin{example}
   Definim $X$ com una variable aleatòria discreta tal que $P(X = 0) = 0$ i
   $P(X = n) = \frac{6}{\pi^2}\cdot \frac{1}{n^2}$. Aleshores
    \[
        G_X(z) =  \frac{6}{\pi^2} \sum_{n \geq 1} \frac{1}{n^2}z^n,
    \]
    que t\'e radi de convergència igual a $1$, i
    \[
        G_X(1) = \frac{6}{\pi^2} \sum_{n \geq 1} \frac{1}{n^2} =
        \frac{6}{\pi^2}\cdot \frac{\pi^2}{6} = 1.
    \]
    Finalment, en calculem la seva esperança,
    \[
        \esp[X] = \eval{\dv{G_X(z)}{z}}_{z = 1} = 
        \frac{6}{\pi^2}\sum_{n \geq 1} \frac{1}{n} = \infty.
    \]
\end{example}

\begin{obs}
    $G_X(z)$ codifica totes les probabilitats $P(X = n)$ i per tant coneixent
    $G_x(z)$ coneixem $X$.
\end{obs}

L'aplicació m\'es útil de les funcions generadores de probabilitat \'es que
ens permet trobar convolucions discretes de variables aleatòries.

\begin{obs}
    Siguin $X, Y$ variables aleatòries discretes, amb $\im(X) = \im(Y) =
    \n_{\geq 0}$. Aleshores
    \[
        P(X+Y = n) = \sum_{k = 0}^{n} P(X+Y = n, X = k) = 
        \sum_{k = 0}^{n} P(Y = n-k, X = k). 
    \]
\end{obs}

\begin{prop}
    Si $X, Y$ són variables aleatòries discretes independents amb $\im(X) = \im(Y) =
    \n_{\geq 0}$ aleshores:
    \[
        G_{X+Y}(z) = G_X(z)G_Y(z)
    \]
\end{prop}

\begin{proof}
    \[
        G_X(z)G_Y(z) = \sum_{i \geq 0} P(X = i) z^i  \sum_{j \geq 0} P(X = j) z^j =
         \sum_{i, j \geq 0} P(X = i)P(Y = j) z^{(i+j)}.
    \]
    I, com $X$ i $Y$ són independents, podem unir el producte i obtenim
    \begin{align*} 
        \sum_{i, j \geq 0} P(X = i, Y = j) z^{(i+j)} & = \sum_{n \geq 0} \sum_{i = 0}^n
        P(X = i, Y = n-i)z^n =\\
        &= \sum_{n \geq 0} \sum_{i = 0}^n P(X = i, X+Y = n)z^n =\\
        & = \sum_{n \geq 0} \sum_{i = 0}^n P(X = i | X+Y = n) P(X+Y = n)z^n = \\
        & = \sum_{n \geq 0} \ P(X+Y = n)z^n \sum_{i = 0}^n P(X = i | X+Y = n).
    \end{align*}
    Si observem que la suma interior val $1$ perquè està sumant la probabilitat de tots
    els esdeveniments possibles, ens queda
    \[
        \sum_{n \geq 0} P(X+Y = n)z^n = G_{X+Y}(z).
    \]
\end{proof}

\begin{obs}
    Això \'es equivalent a que si $X$ i $Y$ són variables aleatòries discretes independents
    amb $\im(X) = \im(Y) = \n_{\geq 0}$ aleshores
    \[
        \esp[z^X]\esp[z^Y] = \esp[z^{X+Y}].
    \]
\end{obs}

\begin{obs}
    En general, si $X_1, \dots, X_n$ són variables aleatòries discretes independents amb
    $\im X_i = \n_{\geq 0}$:
    \[
        G_{X_1,\dots,X_n}(z) = \prod_{i = 1}^n G_{X_i}(z).
    \]
\end{obs}

\section{Models de variables aleatòries discretes}

En aquesta secció introduirem les variables aleatòries discretes més comunes que torbarem
% En aquesta secció hi ha demostracions sense tots els apartats. Això és intencionat, les que queden
% no s'han fet a classe perquè són trivials.

\begin{obs}
    En general escriurem $X \sim Y$ si $X$ i $Y$ tenen la mateixa distribució de probabilitat.
\end{obs}

\subsection*{Distribució de Bernoulli}

Modela l'èxit o fracàs d'un experiment amb probabilitat $p$ d'èxit

\begin{defi}[distribució!de Bernoulli]
    Sigui $X$ una variable aleatoria. Direm que $X$ segueix una distribució de Bernoulli 
    \[X \sim \operatorname{B}(p) \iff \begin{cases}
                       p(X=1) = p \\
                       p(X=0) = 1-p
                       \end{cases}.
    \]
    També es pot escriure $\operatorname{Be}(p)$.
\end{defi}

\begin{prop}
    Sigui $X$ una variable aleatòria que segueix una distribució de Bernoulli. Aleshores,
    \begin{enumerate}[i)]
        \item $G_X(z) = (1-p)z^0 + pz^1 = (1-p) + p(z)$
        \item $\esp[X] = p$
        \item $\var[X] = p(1-p)$
    \end{enumerate}
\end{prop}

\begin{proof}
    \begin{enumerate}[i)]
        \item[]
        \item[iii)] $\esp[x^2-x]= \esp[x(x-1)] = 0 \implies \esp[x^2] = p \implies \var[x] = \esp[x^2] + \esp[x]^2
        = p(1-p)$
    \end{enumerate}
\end{proof}

\subsection*{Distribució binomial}

Modela el nombre d'èxits en fer $N$ experiments independents, on cadascun és $\operatorname{Be}(p)$.

\begin{defi}[distribució!binomial]
  Sigui $X$ una variable aleatoria. Direm que $X$ segueix una distribució binomial
    \[X \sim \operatorname{Bin}(N,p) \iff X = X_1 + \cdots + X_N,\]
    on $\lc X_i \rc_{i=1}^{N}$ són independents i $X_i \sim \operatorname{B}(p) \enspace\forall i$.
\end{defi}

\begin{prop}
    Sigui $X$ una variable aleatòria que segueix una distribució binomial. Aleshores,
    \begin{enumerate}[i)]
        \item $p(X=i) = \binom{N}{i}p^i(1-p)^{N-i}$
        \item $G_X(z) = \sum_{i=0}^{n} \binom{N}{i}p^i(1-p)^{N-i}z^i$
        \item $\esp[X] = N\esp[X_1] = Np$
        \item $\var[X] = N\var[X_1] = Np(1-p)$
    \end{enumerate}
\end{prop}

\begin{proof}
    \begin{enumerate}[i)]
        \item[]
        \item[ii)] Per ser $X_i$ independents,
        \begin{gather*}
        G_X(z) = G_{X_1 + \cdots + X_N}(z) = \prod_{i=1}^{n}G_{X_i}(z) = (pz+(1-p))^N = \\
        = \sum_{i=0}^n \binom{N}{i}(pz)^i(1-p)^{N-i} = \sum_{i=0}^{n} \binom{N}{i}p^i(1-p)^{N-i}z^i.
        \end{gather*}
    \end{enumerate}
\end{proof}

\begin{obs} La suma de dos variables amb aquesta distribució també té distribució binomial:
    \begin{gather*}
    \begin{rcases*} X \sim \operatorname{Bin}(N_1,p) \\ Y \sim \operatorname{Bin}(N_2,p) \end{rcases*} \implies
    \begin{rcases*} G_X(z) = (pz+(1-p))^{N_1} \\ G_Y(z) = (pz+(1-p))^{N_2} \end{rcases*}
    \implies \\
    \implies G_{X+Y}(z) = G_X(z)G_Y(z) = (pz+(1-p))^{N_1+N_2} \implies \\
    \implies X+Y \sim \operatorname{Bin}(N_1+N_2,p).
    \end{gather*}
\end{obs}

\subsection*{Distribució uniforme}

\begin{defi}[distribució!uniforme]
    Sigui $X$ una variable aleatoria. Direm que $X$ segueix una distribució uniforme
    \[X \sim \operatorname{U}[1,n] \iff p(X=i)=\frac{1}{N} \text{ per } i = 1,\dots,N.\]
\end{defi}

\begin{prop} %TODO esp i var les va deixar com a exercici, no sé que fer
    Sigui $X$ una variable aleatòria que segueix una distribució uniforme. Aleshores,
    \begin{enumerate}[i)]
     \item $G_X(z) =  \frac{1}{N}\frac{z(z^N-1)}{z-1}$
     \item $\esp[X]=\frac{N+1}{2}$
     \item $\var[X]=\frac{N^2-1}{12}$
    \end{enumerate}
\end{prop}

\begin{proof}
    Directament de la definició de distribució uniforme tenim que
    \begin{enumerate}[i)]
        \item $G_X(z) = \sum_{n=1}^N \frac{1}{N}z^n = \frac{1}{N}(z+z^2+\cdots+z^N) = \frac{1}{N}\frac{z(z^N-1)}{z-1}$.
        \item $\esp[X]=\sum_{i=0}^N i\frac{1}{N}=\frac{1}{N}\frac{N\lp N+1\rp}{2}=\frac{N+1}{2}$.
        \item
        \begin{align*}
            \var\lb X\rb &= \esp\lb X^2\rb - \esp\lb X\rb ^2 =\\
            &= \lp\sum_{i=0}^N i^2\frac{1}{N}\rp-\lp\frac{N+1}{2}\rp ^2=\\
            &=\frac{\lp N+1\rp \lp 2N+1\rp}{6}-\frac{\lp N+1\rp ^2}{4} =\\
            &= \frac{4N^2+6N+2-\lp 3N^2+6N+3\rp}{12} =\\
            &=\frac{N^2-1}{12}.
        \end{align*}
  \end{enumerate}
\end{proof}


\subsection*{Distribució de Poisson}

S'usa per modelar successos ``estranys'' (persones en una cua, emissió de partícules, etc).

\begin{defi}[distribució!de Poisson]
    Sigui $X$ una variable aleatoria. Direm que $X$ segueix una distribució de Poisson
    \[X \sim \operatorname{P}(\lambda) \iff P(X=i) = \frac{\lambda^i }{i!}e^{-\lambda}.\]
    També es pot escriure $\operatorname{Po}(\lambda)$.
\end{defi}

\begin{obs}
    La distribució està ben definida:
    \[\sum_{i=0}^\infty p(X=i) = \sum_{i=0}^\infty \frac{1}{i!}\lambda^i e^{-\lambda} = e^\lambda e^{-\lambda} = 1.\]
\end{obs}

\begin{prop}
    Sigui $X$ una variable aleatòria que segueix una distribució de Poisson. Aleshores,
    \begin{enumerate}[i)]
        \item $G_X(z) = e^{\lambda(z-1)}$
        \item $\esp[X] = \lambda$
        \item $\var[X] = \lambda$
    \end{enumerate}
\end{prop}

\begin{proof}
    \begin{enumerate}[i)]
        \item[]
        \item $G_X(z) = \sum\limits_{i=0}^\infty \frac{1}{i!}\lambda^i e^{-\lambda} z^i =
            e^{-\lambda} \sum\limits_{i=0}^\infty \frac{1}{i!}(\lambda z)^i = e^{-\lambda}e^{\lambda z} = e^{\lambda(z-1)}$.
        \item $\esp[X] = \lambda e^{\lambda(z-1)}|_{z=1} = \lambda$.
        \item $\esp[X(X-1)] = \lambda^2 e^{\lambda(z-1)}|_{z=1} = \lambda^2 \implies \esp[X^2] = \lambda^2+\lambda$,  i per tant, 
         $\var[X] = \esp[X^2] - \esp[X]^2 = \lambda^2 + \lambda - \lambda^2 = \lambda$.
    \end{enumerate}
\end{proof}

\begin{obs} La suma de dos variables amb distribució de Poisson també té distribució de Poisson:
    \begin{gather*}
    \begin{rcases*} X \sim \operatorname{Po}(\lambda_1) \\ Y \sim \operatorname{Po}(\lambda_2) \end{rcases*} \implies
    \begin{rcases*} G_X(z) = e^{\lambda_1(z-1)} \\ G_Y(z) = e^{\lambda_2(z-1)} \end{rcases*}
    \implies \\
    \implies G_{X+Y}(z) = G_X(z)G_Y(z) = e^{(\lambda_1+\lambda_2)(z-1)} \implies \\
    \implies X+Y \sim \operatorname{Po}(\lambda_1+\lambda_2).
    \end{gather*}
\end{obs}

\subsection*{Distribució geomètrica}
La distribució geomètrica representa el nombre d'experiments necessaris abans d'obtenir el primer èxit en un succés binari.
\begin{defi}[distribució!geomètrica]
    Sigui $X$ una variable aleatoria. Direm que $X$ segueix una distribució geomètrica
    \[X \sim \operatorname{Geom}(p) \iff P(X=i) = \lp 1-p \rp ^{i-1}p,\;\forall i\geq1.\]
\end{defi}

\begin{obs}
    La distribució està ben definida, atès que
    \[
        \sum_{i\geq 1} p\lp1-p\rp^{i-1} = p\sum_{i\geq 1}\lp1-p\rp^{i-1} = p\frac{1}{1-\lp 1-p\rp}=1.
    \]
\end{obs}

\begin{prop}
    Sigui $X$ una variable aleatòria que segueix una distribució geomètrica. Aleshores,
    \begin{enumerate}[i)]
        \item $G_X(z) =  \frac{pz}{1-\lp 1-p\rp z}$
        \item $\esp[X] = \frac{1}{p}$
        \item $\var[X] = \frac{1-p}{p^2}$
    \end{enumerate}
\end{prop}
\begin{proof}
    Directament de les definicions tenim que
    \begin{enumerate}[i)]
        \item $G_X(z) = \sum_{i\geq 1}\lp 1-p \rp ^{i-1}p z^i = \frac{p}{1-p}\sum_{i\geq 1}\lp\lp 1-p \rp z\rp^{i} = 
        \frac{p}{1-p}\cdot\frac{\lp 1-p \rp z}{1-\lp 1-p\rp z}= \frac{pz}{1-\lp 1-p\rp z}$.
        \item 
            \begin{align*}
                \esp\lb X\rb &= \lp \frac{\dif G_X\lp X\rp}{\dif z}\rp \lp 1\rp =\\
                &=\lp\frac{p\lp 1-\lp 1-p\rp z\rp+p\lp 1-p\rp z}{\lp 1-\lp 1-p\rp z\rp ^2}\rp\lp 1\rp =\\
                &=\frac{p^2+p-p^2}{p^2}=\\
                &=\frac{1}{p}.
            \end{align*}
        \item Es pot fer igual que en la distribució anterior: calculant $\esp\lb X\lp X-1\rp\rb$ amb la segona derivada de $G_X\lp z\rp$ i utilitzant que $\var\lb X\rb = \esp\lb X^2\rb - \esp\lb X\rb ^2$, però les expressions són farragoses i no ho farem.
    \end{enumerate}
\end{proof}

\subsection*{Distribució binomial negativa}

Modela el nombre d'experiments necessaris per aconseguir un nombre d'èxits $r$ donat.

\begin{defi}[distribució!binomial negativa]
    Sigui $X$ una variable aleatoria. Direm que $X$ segueix una distribució binomial negativa
    \[X \sim \operatorname{BinN}(r,p) \iff X = X_1 + \cdots + X_r,\]
    on $\lc X_i \rc_{i=1}^{r}$ són independents i $X_i \sim \operatorname{Geom}(p) \enspace\forall i$.
\end{defi}

\begin{obs}
    Una distribució binomial negativa és equivalent a $r$ distribucions geomètriques seguides.
\end{obs}


\begin{prop}
    Sigui $X$ una variable aleatòria que segueix una distribució binomial negativa. Aleshores,
    \begin{enumerate}[i)]
        \item $G_X(z) = \lp \frac{pz}{1-\lp 1-p\rp z}\rp^r$
        \item $p(X=k) = \begin{cases}
                         0 &\text{ si } k < r\\
                         \binom{k-1}{r-1}p^{r}(1-p)^{k-r} & \text{ si }k\geq r
                        \end{cases}$
        \item $\esp[X] = r\esp[X_1] = \frac{r}{p}$
        \item $\var[X] = r\var[X_1] = r\frac{1-p}{p^2}$
    \end{enumerate}
\end{prop}

\begin{proof}
    \begin{enumerate}[i)]
        \item[]
        \item $G_X(z) = (G_{X_1}(z))^r = \lp \frac{pz}{1-\lp 1-p\rp z}\rp^r$
        \item $p(X=i) = \binom{k-1}{r-1}p^{r-1}(1-p)^{k-1-(r-1)} = \binom{k-1}{r-1}p^{r}(1-p)^{k-r}$
        %TODO Això últim no sé d'on ix i tinc massa son per pensar-ho
    \end{enumerate}
\end{proof}

\section{Distribucions condicionades i esperança condicionada}

\begin{defi}[funció!de distribució!condicionada]\idx{funció!de probabilitat!condicionada}
Siguin $X$ i $Y$ variables aleatòries discretes i sigui $x\in\real$ tal que $p\lp X=x\rp >0$. Aleshores definim
    \begin{enumerate}[1)]
        \item $F_{Y\vert X} \lp y,x\rp=p\lp Y\geq y\mid X=x \rp$ com la funció de distribució condicionada de $Y$ amb $X=x$,
        \item $P_{Y\vert X} \lp y,x\rp=p\lp Y = y\mid X=x \rp$ com la funció de probabilitat condicionada.
    \end{enumerate} 
\end{defi}

\begin{obs}
 Una definició anàloga consisteix en prendre $A\in\Asuc$ enlloc de $X=x$ sempre que $p(A)>0$. Aleshores tenim
    \begin{enumerate}[1)]
        \item $F_{Y\vert X} \lp y,x\rp=p\lp Y\geq y\mid A \rp$
        \item $P_{Y\vert X} \lp y,x\rp=p\lp Y = y\mid A \rp$
    \end{enumerate} 
\end{obs}

\begin{example}
    Siguin $\left\{Y_r\right\}_{r\geq 1}$ variables aleatòries independents tals que $Y_r \sim \operatorname{Be}(p)\; \forall i$. Sigui $X$ una 
    variable aleatòria tal que $X=i$ si $Y_1 = Y_2 = \cdots = Y_{i-1}=0$ i $Y_i=1$. Observem que $X\mid\{Y_1=1\}= X\vert_{Y_1=1}=1$, 
    $X\vert_{Y_1=0}=1+\operatorname{Geom}(p)$ i $X\vert_{Y_1=0,Y_2=0}=2+\operatorname{Geom}(p)$.
\end{example}

\begin{defi}[esperança!condicionada]
    Siguin $X,Y$ v.a. discretes, i sigui $x$ tal que $P(X=x)>0$. L'esperança condicionada de $Y$ a $X=x$ és
    \[\esp[Y \vert X=x] = \sum_{y \in \im(Y)} y \cdot P(Y=y \vert X=x) = \phi(x)\]
    Observem que, per cada tria de $x\in\im(X), \phi(x) = \esp[Y \vert X=x]$ té un valor diferent.
\end{defi}

\begin{defi}[esperança!condicionada]
    Siguin $X,Y$ v.a. discretes,
    \[\esp[Y \vert X] = \phi(x) \text{ amb probabilitat } P(X=x)\],
    sumant els valors de $X$ que fan igual $\phi(x)$,
\end{defi}

\begin{example}
    \[\begin{rcases}
    P(X=1) = \frac{1}{4}, \phi(1)=2\\
    P(X=2) = \frac{1}{4}, \phi(2)=2
    \end{rcases} \implies P(\esp[Y \vert X]=2)=\frac{1}{4}+\frac{1}{4}\]
\end{example}

\begin{obs}
    $\phi(x) = \esp[Y \vert X=x]$ és una funció sobre $x$ amb valors reals, en canvi $\esp[Y \vert X]$ és una variable aleatòria.
\end{obs}

\begin{prop}
    \[\esp[\esp[Y \vert X] = \esp[Y]\]
\end{prop}

\begin{proof}
    \begin{align*}
    \esp[\esp[Y \vert X] = & \sum_{x\in\im(X)} \phi(x)P(X=x) = \\
    = & \sum_{x\in\im(X)} \sum_{y\in\im(Y)} y\phi(x)P(Y=y \vert X=x)P(X=x) = \\
    = & \sum_{y\in\im(Y)} \sum_{x\in\im(X)} y\phi(x)P(Y=y \vert X=x)P(X=x) = \\
    = & \sum_{y\in\im(Y)} y\sum_{x\in\im(X)} \phi(x)P(Y=y,X=x) = \\
    = & \sum_{y\in\im(Y)} y P(Y=y) = \esp[Y]
    \end{align*}
\end{proof}


