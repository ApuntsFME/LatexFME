\chapter{Ceros de funciones}

Los métodos numéricos nos proporcionan aproximaciones succesivas de una raíz ($\alpha$) de una función.
Comenzaremos con una aproximación inicial $x^0$  y el método nos proporcionará $x^1$. Más
generalmente, dado un $x^k$, el método nos proporcionará $x^{k+1}$.

\section{Métodos clásicos}

\subsection{Método de la bisección}

El método está basado en el teorema de Bolzano. Consiste en tomar un intervalo $(a, b)$
con $f(a)f(b) < 0$. Una vez encontrado este intervalo inicial, vamos dividiendo sucesivamente
el intervalo por la mitad y tomando el subintervalo que cumpla la condición de que sus extremos tengan
distinto signo. Reduciendo el error a la mitad.

Este método tiene la ventaja de ser muy robusto y nos permite calcular la raíz con una percisión arbitraria.
El inconveniente principal de este método es la velocidad: es un método muy lento.

\subsection{Método de Newton}

Dada una aproximación de la raíz $x^k$, calcularemos la siguiente aproximación $x^{k+1}$ como el cero de 
la recta tangente a $f$ en $x^k$, es decir, aproximaremos $f(x) \approx
f\left( x^k \right) + f^\prime\left( x^k \right) \left( x - x^k \right)$ y $x^{k+1}$ será el 0 de esta función aproximada:

\[
    x^{k+1} = x^k - \frac{f\left( x^k \right)}{f^\prime\left( x^k \right)}.
\]

La ventaja de este método es la velocidad (tiene convergencia cuadrática para raíces simples).
El inconveniente principal es que no es tan robusto: necesita una aproximación inicial suficientemente buena.
Además, este método requiere de conocer la derivada $f^\prime(x)$, lo cual puede ser costoso o imposible.

\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
                xmin = 0.5,
                xmax = 2,
                ymin = -0.5,
                ymax = 0.5,
                samples = 100,
                axis lines = center
        ]
            \addplot[domain=0.5:2, thick]{1/x-1};
            \addplot[mark=*,color=gray] coordinates {(0.8, 1/0.8 - 1)} node[]{};
            \addplot[mark=*,color=red] coordinates {(0.8, 0)} node[label=120:$x^0$]{};
            \addplot[domain=0.5:2, thick]{1/0.8 - 1 + (x - 0.8)*(-1/(0.8*0.8))};
            \addplot[mark=*, color=blue] coordinates {(0.8 - (1/0.8 - 1)/(-1/(0.8*0.8)), 0)} node[pin=90:$x^1$]{};
            \addplot[mark=*, color=teal] coordinates {(1, 0)} node[label=80:$\alpha$]{};
            
	    \draw (axis cs:0.8, 1/0.8 - 1) [color=gray, dashed] -- (axis cs:0.8, 0);
        
        \end{axis}
    \end{tikzpicture}
\end{center}

\subsection{Método de la secante}

Este método parte de dos aproximaciones iniciales $x^0$ y $x^1$, tomaremos $x^2$ como el cero de la recta
que pasa por $x^0$ y $x^1$ (substituye la derivada por la recta secante). Es decir, dados $x^k$ y $x^{k-1}$,
\[
    x^{k+1} = x^k - f\left( x^k \right) \frac{x^k - x^{k-1}}{f\left( x^k \right) - f\left( x^{k-1} \right)}.
\]

Las ventajas e inconvenientes de este método son casi identicas a las del método de newton. Este método
es un poco más lento que el de Newton, aunque converge bastante rápidamente y, igual que el de Newton,
el punto flaco es que no es tan robusto como el de la bisección.

\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
                xmin = 0.2,
                xmax = 2,
                ymin = -0.5,
                ymax = 0.5,
                samples = 100,
                axis lines = center
        ]
            \addplot[domain=0.5:2, thick]{1/x-1};
            \addplot[mark=*,color=gray] coordinates {(0.7, 1/0.7 - 1)} node[]{};
            \addplot[mark=*,color=gray] coordinates {(1.4, 1/1.4 - 1)} node[]{};
            
            \addplot[mark=*,color=red] coordinates {(0.7, 0)} node[label=270:$x^0$]{};
            \addplot[mark=*,color=blue] coordinates {(1.4, 0)} node[label=90:$x^1$]{};
            \addplot[domain=0.5:2, thick]{-50/49*x + 56/49};
            \addplot[mark=*, color=green] coordinates {(56/50, 0)} node[label=85:$x^2$]{};
            \addplot[mark=*, color=teal] coordinates {(1, 0)} node[label=90:$\alpha$]{};
            
            \draw (axis cs:0.7, 1/0.7 - 1) [color=gray, dashed] -- (axis cs:0.7, 0);
            \draw (axis cs:1.4, 1/1.4 - 1) [color=gray, dashed] -- (axis cs:1.4, 0);
        \end{axis}
    \end{tikzpicture}
\end{center}

\begin{obs*}
    El coste computacional se mide en número de evaluaciones de $f(x)$ (i de $F^\prime(x)$).
    El método de la secante necesita 1 evaluación por iteración y por lo tanto, necesita menos que
    el método de Newton.
\end{obs*}

\section{Consistencia y convergencia}

\begin{defi}[método!consistente]
    Diremos que un método $x^{k+1} = \phi\left( x^k \right)$ es consistente si las raíces son
    los únicos puntos fijos, es decir,
    \[
        f(\alpha) = 0 \iff \phi(\alpha) = \alpha.
    \]
\end{defi}

\begin{ej}
    Un ejemplo es el método de Newton.

    Si $\alpha$ es una raíz simple,
    \[
        f(\alpha) = 0 \implies \phi(\alpha) = \alpha - \frac{f(\alpha)}{f^\prime(\alpha)} = \alpha.
    \]
    Por otro lado,
    \[
        \phi(\alpha) = \alpha \implies \frac{f(\alpha)}{f^\prime(\alpha)} = 0 \implies f(\alpha) = 0.
    \]
    Con la raíz doble, no podemos evaluar el método de Newton en $\alpha$, pero por la regla de l'Hôpital llegaríamos
    a la conclusión de que, tomando el límite de $\phi$ se sigue cumpliendo.

    Análogamente, podemos deducir que el método de Newton es un método consistente para una raíz $\alpha$ de
    cualquier múltiplicidad.
\end{ej}

\begin{defi}[método!convergente]
    Diremos que un esquema es convergente si
    \[
        \lim_{k \to \infty} x^k = \alpha.
    \]
\end{defi}

\begin{obs}
    Llamaremos error de la aproximación $E^k$ a 
    \[
        E^k = \abs{x^k - \alpha}
    \]
    y es equivalente que el método sea convergente a que $\lim\limits_{k \to \infty} E^k = 0$.
\end{obs}

\begin{defi}[convergencia!lineal]
    Diremos que un esquema tiene convergencia lineal si
    \[
        \abs{E^{k+a}} \leq \lambda \abs{E^k}, \; \lambda < 1.
    \]
\end{defi}

\begin{defi}[convergencia!de orden $p$]
    Diremos que un esquema tiene convergencia de orden $p$ si
    \[
        \abs{E^{k+1}} \leq \lambda \abs{E^k}^p.
    \]
\end{defi}

\begin{defi}[convergencia!superlineal]
    Diremos que un esquema tiene convergencia superlineal si
    \begin{gather*}
        \abs{E^{k+a}} \leq \lambda_k \abs{E^k}, \; \lambda_k \to 0 \\
        \text{o bien} \\
        \abs{E^{k+1}} \leq \lambda \abs{E^k}^q, \; q \in (1, 2).
    \end{gather*}
\end{defi}
